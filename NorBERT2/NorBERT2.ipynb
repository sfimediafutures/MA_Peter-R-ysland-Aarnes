{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import conllu\n",
    "import tqdm\n",
    "import torch\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional\n",
    "from functools import partial\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a lot of inspo from https://github.com/ltgoslo/NorBERT/tree/main/benchmarking/experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Could tidy up this one to just use device instead of calling gpu=true etc.\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    gpu = True\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conllu stuff\n",
    "def filter_tags(x):\n",
    "    return x        \n",
    "\n",
    "def convert_to_list_dict(path, file):\n",
    "    path = path.format(file)\n",
    "    with open(path, encoding=\"UTF-8\") as infile:\n",
    "        lst = []\n",
    "        tokens = list(conllu.parse_incr(infile))\n",
    "        for sent in tokens:\n",
    "                dic = {\n",
    "                \"idx\": sent.metadata[\"sent_id\"],\n",
    "                \"text\": sent.metadata[\"text\"].lower(),\n",
    "                \"tokens\": [token[\"form\"].lower() for token in sent],\n",
    "                \"lemmas\": [token[\"lemma\"] for token in sent],\n",
    "                \"pos_tags\": [token[\"upos\"] for token in sent],\n",
    "                \"ner_tags\": [filter_tags(token[\"misc\"].get(\"name\", \"O\")) for token in sent],\n",
    "            }\n",
    "                lst.append(dic) \n",
    "        print(\"Converting {} to list of dictionaries\\n     {} elements converted..\".format(file, len(lst)))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/dataset.py\n",
    "\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, x_tokens, y_labels, ner_vocab=None):\n",
    "        self.tokens = [[x for x in entry] for entry in x_tokens]\n",
    "        self.ner_labels = [[y for y in entry] for entry in y_labels]\n",
    "\n",
    "        # hard coded ner_vocab to avoid random shuffled instanciation of ordering of ner_vocab\n",
    "        self.ner_vocab = ner_vocab\n",
    "        self.ner_indexer = {i: n for n, i in enumerate(self.ner_vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.tokens[index]\n",
    "        ner_labels = self.ner_labels[index]\n",
    "\n",
    "        x = tokens\n",
    "        y = torch.LongTensor([self.ner_indexer[i] if i in self.ner_vocab\n",
    "                              else self.ner_indexer['@UNK'] for i in ner_labels])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dynamic padding. Takes the longest sentence in batch and pads other sentences to its length (if im not mistaken)\"\"\"\n",
    "# Function borrowed from https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/bert_ner.py\n",
    "\n",
    "def collate_fn(batch, gpu=False):\n",
    "    longest_y = max([y.size(0) for X, y in batch])\n",
    "    x = [X for X, y in batch]\n",
    "    y = torch.stack(\n",
    "        [functional.pad(y, (0, longest_y - y.size(0)), value=-1) for X, y in batch]) #https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
    "    if gpu:\n",
    "        y = y.to(\"cuda\")\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorBert(nn.Module):\n",
    "    def __init__(self, ner_vocab, model_path=None, freeze=False):\n",
    "        super().__init__()\n",
    "        self._bert = BertModel.from_pretrained(\n",
    "            model_path\n",
    "        )\n",
    "        hidden_size = self._bert.config.hidden_size\n",
    "        self._linear = nn.Linear(hidden_size, len(ner_vocab))\n",
    "\n",
    "        if freeze:\n",
    "            for param in self._bert.parameters():\n",
    "                param.requires_grad = False #Freezing bert layer\n",
    "\n",
    "    def forward(self, batch, mask):\n",
    "        b = self._bert(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        pooler = b.last_hidden_state[:, mask].diagonal().permute(2, 0, 1) #https://pytorch.org/docs/stable/generated/torch.permute.html\n",
    "        return self._linear(pooler)                                     #https://pytorch.org/docs/stable/generated/torch.diagonal.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/bert_ner.py \n",
    "\n",
    "def build_mask(tokenizer, ids):\n",
    "    tok_sents = [tokenizer.convert_ids_to_tokens(i) for i in ids]\n",
    "    mask = []\n",
    "    for sentence in tok_sents:\n",
    "        current = []\n",
    "        for n, token in enumerate(sentence):\n",
    "            if token in tokenizer.all_special_tokens[1:] or token.startswith(\"##\"): # ## masked\n",
    "                continue\n",
    "            else:\n",
    "                current.append(n)\n",
    "        mask.append(current)\n",
    "\n",
    "    mask = tokenizer.pad({\"input_ids\": mask}, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, tokenizer, model, gpu=False):\n",
    "    input_data = tokenizer(\n",
    "        input_data, is_split_into_words=True, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    if gpu:\n",
    "        input_data = input_data.to(\"cuda\")\n",
    "    batch_mask = build_mask(tokenizer, input_data[\"input_ids\"])\n",
    "    y_pred = model(input_data, batch_mask).permute(0, 2, 1).argmax(dim=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, train_loader, dev_loader, tokenizer, model, optimiser, scheduler, criterion, no_ne_index, MAX_EPOCHS, OUTPUT_PATH, PATIENCE):\n",
    "    \n",
    "    accuracy_lst = []\n",
    "    loss_history_train, loss_history_dev, val_accuracy = {}, {}, {}\n",
    "    inpatience = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # TRAIN\n",
    "        loss_accumulation = 0\n",
    "        model.train()\n",
    "        train_iter = tqdm.tqdm(train_loader)\n",
    "        for x, y in train_iter:\n",
    "            x = tokenizer(\n",
    "                x, is_split_into_words=True, return_tensors=\"pt\", padding=True\n",
    "            )\n",
    "            if gpu:\n",
    "                x = x.to(device)\n",
    "            batch_mask = build_mask(tokenizer, x[\"input_ids\"])\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(x, batch_mask).permute(0, 2, 1)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss_accumulation+=loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_iter.set_postfix_str(f\"loss: {loss.item()}\")\n",
    "                \n",
    "        # EVAL       \n",
    "        model.eval()\n",
    "        dev_iter = tqdm.tqdm(dev_loader)\n",
    "        correct, total = 0, 0\n",
    "        for x, y in dev_iter:\n",
    "            y_pred = predict(x, tokenizer, model, gpu=gpu)\n",
    "            correct += (((y_pred == y).logical_and(y != no_ne_index)).nonzero(as_tuple=False).size(0))\n",
    "            total += ((y != no_ne_index).logical_and(y != -1).nonzero(as_tuple=False).size(0))\n",
    "            x = tokenizer(\n",
    "                x, is_split_into_words=True, return_tensors=\"pt\", padding=True\n",
    "            )\n",
    "            if gpu:\n",
    "                x = x.to(device)\n",
    "            batch_mask = build_mask(tokenizer, x[\"input_ids\"])\n",
    "            y_pred = model(x, batch_mask).permute(0, 2, 1)\n",
    "            dev_loss = criterion(y_pred, y)\n",
    "            scheduler.step(dev_loss)\n",
    "\n",
    "        print(\"Epoch number: {}\".format(epoch))\n",
    "        accuracy = correct / total\n",
    "        accuracy_lst.append(accuracy)\n",
    "        current_acc_best = max(accuracy_lst)\n",
    "        print(accuracy)\n",
    "        print(f\"List of accuracies: {accuracy_lst}\")\n",
    "        print(f\"Best accuracy so far: {current_acc_best}\")\n",
    "        print(f\"Validation accuracy = {correct} / {total} = {accuracy}\")\n",
    "        if ((accuracy < current_acc_best) and (inpatience <= PATIENCE)): \n",
    "            print(f\"No improvement. Inpatience counter increased..\")\n",
    "            print(f\"Patience set at {PATIENCE}\")\n",
    "            inpatience +=1\n",
    "            print(f\"Inpatience counter reached {inpatience}\")\n",
    "            if inpatience == PATIENCE:\n",
    "                print(f\"Patience tolerance reached. Early stopping at epoch {epoch}!\")\n",
    "                break\n",
    "        else:\n",
    "            current_acc_best = accuracy\n",
    "            print(\"New best!\")\n",
    "            print(\"Setting inpatience counter to 0\")\n",
    "            inpatience = 0\n",
    "            torch.save(model.state_dict(), OUTPUT_PATH)\n",
    "\n",
    "        loss_history_train[\"epoch: {}\".format(epoch)] = loss_accumulation\n",
    "        loss_history_dev[\"epoch: {}\".format(epoch)] = loss_accumulation\n",
    "        val_accuracy[\"epoch: {}\".format(epoch)] = accuracy\n",
    "        \n",
    "\n",
    "    return loss_history_train, loss_history_dev, val_accuracy, epoch\n",
    "\n",
    "# \"\"\" TODO: legg til tekst fil med loss og andre stats\"\"\"\n",
    "# INTERGRATE TODO PATIENCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test_set, ner_vocab, tokenizer, model):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    test_set = tqdm.tqdm(test_set)\n",
    "    for x, y in test_set:\n",
    "        y_pred = predict(x, tokenizer, model, gpu=gpu)\n",
    "        predicted = [ner_vocab[element] for element in y_pred[0]]\n",
    "        predicted_labels += predicted\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_freeze(freeze, model):\n",
    "    if freeze:\n",
    "        lr=0.001\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Learn rate {}\".format(lr))\n",
    "    else:\n",
    "        lr = 2e-5\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Learn rate {}\".format(lr))\n",
    "    return optimiser, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_info_json(model_name, loss_history_train, loss_history_dev, val_accuracy, MAX_EPOCHS, stop_epoch, lr, seed, PATIENCE):\n",
    "    info = {\n",
    "        \"Seed\": seed,\n",
    "        \"Max epochs set\": MAX_EPOCHS,\n",
    "        \"Train stop at epoch (begins at 0)\": stop_epoch,\n",
    "        \"Learning rate\": lr,\n",
    "        \"Validation accuracy history\": val_accuracy,\n",
    "        \"Train loss history\": loss_history_train,\n",
    "        \"Val loss history\": loss_history_dev,\n",
    "        \"Patience set\": PATIENCE\n",
    "    }\n",
    "\n",
    "    file_name = \"{}_seed_{}.json\".format(model_name, seed)\n",
    "    with open(file_name, \"w\") as write:\n",
    "        json.dump(info, write, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting no_bokmaal-ud-dev to list of dictionaries\n",
      "     2410 elements converted..\n",
      "Converting no_bokmaal-ud-test to list of dictionaries\n",
      "     1939 elements converted..\n",
      "Converting no_bokmaal-ud-train to list of dictionaries\n",
      "     15696 elements converted..\n",
      "Converting no_nynorsk-ud-dev to list of dictionaries\n",
      "     1890 elements converted..\n",
      "Converting no_nynorsk-ud-test to list of dictionaries\n",
      "     1511 elements converted..\n",
      "Converting no_nynorsk-ud-train to list of dictionaries\n",
      "     14174 elements converted..\n",
      "Combining train, dev and test sets..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "path = \"all_conllu/{0}.conllu\"\n",
    "file_list = [\"no_bokmaal-ud-dev\", \"no_bokmaal-ud-test\", \"no_bokmaal-ud-train\", \"no_nynorsk-ud-dev\", \"no_nynorsk-ud-test\", \"no_nynorsk-ud-train\"]\n",
    "\n",
    "dev_split_no = convert_to_list_dict(path, file_list[0])\n",
    "test_split_no = convert_to_list_dict(path, file_list[1])\n",
    "train_split_no = convert_to_list_dict(path, file_list[2])\n",
    "\n",
    "dev_split_ny = convert_to_list_dict(path, file_list[3])\n",
    "test_split_ny = convert_to_list_dict(path, file_list[4])\n",
    "train_split_ny = convert_to_list_dict(path, file_list[5])\n",
    "\n",
    "print(\"Combining train, dev and test sets..\")\n",
    "dev_split = dev_split_no + dev_split_ny\n",
    "test_split = test_split_no + test_split_ny\n",
    "train_split = train_split_no + train_split_ny\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_vocab =     ['B-GPE_LOC', 'I-DRV', 'I-LOC', 'B-PER', 'I-PER', 'B-PROD', \n",
    "                'I-GPE_ORG', 'B-GPE_ORG', 'B-EVT', 'B-DRV', 'I-PROD', 'B-ORG', 'B-MISC',\n",
    "                'I-MISC', 'I-GPE_LOC', 'B-LOC', 'I-ORG', 'I-EVT', 'O', '@UNK']\n",
    "\n",
    "x_train_tokens = [x[\"tokens\"] for x in train_split]\n",
    "y_train_labels = [y[\"ner_tags\"] for y in train_split]\n",
    "train_dataset = CoNLLDataset(x_train_tokens, y_train_labels, ner_vocab)\n",
    "\n",
    "x_dev_tokens = [x[\"tokens\"] for x in dev_split]\n",
    "y_dev_labels = [y[\"ner_tags\"] for y in dev_split]\n",
    "dev_dataset = CoNLLDataset(x_dev_tokens, y_dev_labels, ner_vocab)\n",
    "\n",
    "x_test_tokens = [x[\"tokens\"] for x in test_split]\n",
    "y_test_labels = [y[\"ner_tags\"] for y in test_split]\n",
    "test_dataset = CoNLLDataset(x_test_tokens, y_test_labels, ner_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping data for train and predict\n",
    "train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, collate_fn=partial(collate_fn, gpu=gpu))\n",
    "dev_loader = DataLoader(\n",
    "        dev_dataset, batch_size=32, shuffle=False, collate_fn=partial(collate_fn, gpu=gpu))\n",
    "test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=partial(collate_fn, gpu=gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-GPE_LOC',\n",
       " 'I-DRV',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-PROD',\n",
       " 'I-GPE_ORG',\n",
       " 'B-GPE_ORG',\n",
       " 'B-EVT',\n",
       " 'B-DRV',\n",
       " 'I-PROD',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'I-MISC',\n",
       " 'I-GPE_LOC',\n",
       " 'B-LOC',\n",
       " 'I-ORG',\n",
       " 'I-EVT',\n",
       " 'O',\n",
       " '@UNK']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.ner_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training block \"\"\"\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "no_ne_index = train_dataset.ner_indexer[\"O\"] # For train parameter\n",
    "model_path = \"ltgoslo/norbert2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_basic_tokenize=False)\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "seeds = [8, 37, 42, 101, 1024]\n",
    "freeze = False\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Shuffling dataset for each seed. When taking mean average from each seed, hopefully the\n",
    "    # results will be somewhat the same even if the train data is shuffled.\n",
    "\n",
    "    # Instaniating model\n",
    "    norbert2 = NorBert(ner_vocab, model_path, freeze).to(device)\n",
    "    model_name = \"norbert2\"\n",
    "\n",
    "    # Some paramters for train\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    optimiser, lr = is_freeze(freeze, norbert2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min',\n",
    "            factor=0.1, patience=2, threshold=1e-7, threshold_mode='abs')\n",
    "            \n",
    "    OUTPUT_PATH = \"norbert2_model_adam_lr_{}__maxEpochs_{}__seed_{}\".format(lr, MAX_EPOCHS, seed)\n",
    "    loss_history_train, loss_history_dev, val_accuracy, stop_epoch = train(\n",
    "                                        device, train_loader, dev_loader, \n",
    "                                        tokenizer, norbert2, optimiser, scheduler, criterion,\n",
    "                                        no_ne_index, MAX_EPOCHS, OUTPUT_PATH, PATIENCE\n",
    "                                        )\n",
    "\n",
    "    info = model_info_json(model_name, loss_history_train, loss_history_dev, val_accuracy,\n",
    "                         MAX_EPOCHS, stop_epoch, lr, seed, PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gold labels for NorNE test_dataset\"\"\"\n",
    "\n",
    "gold_labels = []\n",
    "for sentence_labels in test_dataset.ner_labels:\n",
    "    for label in sentence_labels:\n",
    "        gold_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltgoslo/norbert2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"ltgoslo/norbert2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_basic_tokenize=False)\n",
    "\n",
    "MODEL_LOAD_PATH = \"\"\n",
    "\n",
    "bertbert = NorBert(ner_vocab, model_path, freeze).to(device)\n",
    "bertbert.load_state_dict(torch.load(MODEL_LOAD_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3450/3450 [00:33<00:00, 104.22it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = predict_test(test_loader, ner_vocab, tokenizer, bertbert) #train data for ner_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-GPE_LOC       0.84      0.93      0.88       428\n",
      "       I-DRV       0.78      0.54      0.64        13\n",
      "       I-LOC       0.78      0.62      0.69        85\n",
      "       B-PER       0.92      0.93      0.92       961\n",
      "       I-PER       0.92      0.98      0.95       510\n",
      "      B-PROD       0.69      0.31      0.43       131\n",
      "   I-GPE_ORG       1.00      0.57      0.73         7\n",
      "   B-GPE_ORG       0.82      0.54      0.65        61\n",
      "       B-EVT       0.36      0.36      0.36        14\n",
      "       B-DRV       0.77      0.76      0.76        78\n",
      "      I-PROD       0.70      0.36      0.47       126\n",
      "       B-ORG       0.77      0.83      0.80       521\n",
      "      B-MISC       0.00      0.00      0.00        14\n",
      "      I-MISC       0.00      0.00      0.00         3\n",
      "   I-GPE_LOC       0.97      0.91      0.94        79\n",
      "       B-LOC       0.74      0.66      0.70       185\n",
      "       I-ORG       0.69      0.74      0.71       248\n",
      "       I-EVT       0.07      0.25      0.11         4\n",
      "\n",
      "   micro avg       0.84      0.82      0.83      3468\n",
      "   macro avg       0.66      0.57      0.60      3468\n",
      "weighted avg       0.83      0.82      0.82      3468\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cr = confusion_matrix(gold_labels, preds)\n",
    "print(classification_report(gold_labels, preds, labels = ner_vocab[:-2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from seqeval.scheme import IOB2\n",
    "# from seqeval.metrics import classification_report as cr\n",
    "\n",
    "# classification_rep = cr(gold_labels, preds)\n",
    "# print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "# from sklearn import metrics\n",
    "# import itertools\n",
    "\n",
    "\n",
    "# score = metrics.accuracy_score(gold_labels, predicted_tags)\n",
    "# print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "# cm = metrics.confusion_matrix(gold_labels, predicted_tags)\n",
    "# plot_confusion_matrix(cm, classes=set(gold_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.get_parl_corpus_to_dict import ParlamentaryCorpus\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score, precision_score, recall_score, classification_report\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_parl(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for item in corpus:\n",
    "        for value in item.values():\n",
    "            tokenized_corpus.append((word_tokenize(value)))\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_parl_from_iob2(array):\n",
    "    temp_lst = []\n",
    "    for tag in array:\n",
    "        if tag == \"O\":\n",
    "            temp_lst.append(0)\n",
    "        else:\n",
    "            temp_lst.append(1)\n",
    "    return temp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = r\"parl_corpus\\20170207_sentence_data.json\"\n",
    "path2 = r\"parl_corpus\\20170110_sentence_data.json\"\n",
    "\n",
    "c1 = ParlamentaryCorpus(path1)\n",
    "c2 = ParlamentaryCorpus(path2)\n",
    "c1_gold = c1.load_data()\n",
    "c2_gold = c2.load_data()\n",
    "lowered_corpus1 = c1.load_data(lower=True)\n",
    "lowered_corpus2 = c2.load_data(lower=True)\n",
    "\n",
    "lowered_corpus = lowered_corpus1 + lowered_corpus2\n",
    "gold_corpus = c1_gold + c2_gold\n",
    "\n",
    "tokens_lower_corpus = tokenize_parl(lowered_corpus)\n",
    "token_gold_corpus = tokenize_parl(gold_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2589"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Provides dummy y-data so CoNLLDataset class can be used \"\"\"\n",
    "\"\"\" Not elegant but works perfectly fine :)                 \"\"\"\n",
    "\n",
    "dummy_y_parl = []\n",
    "for sentence in tokens_lower_corpus:\n",
    "    temp = []\n",
    "    temp.extend(len(sentence)*\"O\")\n",
    "    dummy_y_parl.append(temp)\n",
    "\n",
    "len(dummy_y_parl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parl_lower_corpus = CoNLLDataset(tokens_lower_corpus, dummy_y_parl, ner_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parl_loader = DataLoader(\n",
    "        parl_lower_corpus, batch_size=1, shuffle=False, collate_fn=partial(collate_fn, gpu=gpu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2589/2589 [00:25<00:00, 100.24it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_parl = predict_test(parl_loader, ner_vocab, tokenizer, bertbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prl = binary_parl_from_iob2(prediction_parl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Annotating binary gold labels for parl corpus\"\"\"\n",
    "\n",
    "length = 0\n",
    "gold_annotations = []\n",
    "for sentences in token_gold_corpus:\n",
    "    annotation = []\n",
    "    length += len(sentences)\n",
    "    for token in sentences:\n",
    "        if token[0].isupper() == True: # Checks if token has captial letter\n",
    "            annotation.append(1)\n",
    "            \n",
    "        else:\n",
    "            annotation.append(0) # Adds 0 if token has lower \n",
    "    gold_annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_parl = [item for sublist  in gold_annotations for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44912   582]\n",
      " [   98  1995]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     45494\n",
      "           1       0.77      0.95      0.85      2093\n",
      "\n",
      "    accuracy                           0.99     47587\n",
      "   macro avg       0.89      0.97      0.92     47587\n",
      "weighted avg       0.99      0.99      0.99     47587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true_parl, prl))\n",
    "print(classification_report(y_true_parl, prl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stortingets', 'møte', 'er', 'lovlig', 'satt', 'representantene', 'fredric', 'helen', 'fredric', 'holen', 'bjørdal', 'og', 'trond', 'giske', 'som']\n",
      "['B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O']\n",
      "\n",
      "Sentence with capitalization prediction:\n",
      "Stortingets møte er lovlig satt representantene Fredric Helen Fredric Holen Bjørdal og Trond Giske som har vært permitterte har igjen tatt sete\n"
     ]
    }
   ],
   "source": [
    "# Example predictions\n",
    "some_tokens = []\n",
    "for t in parl_lower_corpus.tokens[:2]:\n",
    "    for x in t:\n",
    "        some_tokens.append(x)\n",
    "print(some_tokens[:15])\n",
    "print(prediction_parl[:15])\n",
    "\n",
    "cap_sent = []\n",
    "for token, pred in zip(some_tokens, prediction_parl):\n",
    "    if pred != \"O\":\n",
    "        cap_sent.append(token.capitalize())\n",
    "    else:\n",
    "        cap_sent.append(token)\n",
    "\n",
    "print(\"\\nSentence with capitalization prediction:\")\n",
    "print(\" \".join(cap_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e5f1d1d80de0e037045f44f922af48a35ae55538dc259ba83b72568cdb122e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
