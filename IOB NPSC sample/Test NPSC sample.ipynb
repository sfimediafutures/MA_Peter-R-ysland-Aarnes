{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import tqdm\n",
    "import torch\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional\n",
    "from functools import partial\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils.get_parl_corpus_token_data import ParlamentaryCorpus\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import classification_report as cr\n",
    "from seqeval.metrics import performance_measure\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import performance_measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Could tidy up this one to just use device instead of calling gpu=true etc.\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    gpu = True\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conllu stuff\n",
    "def filter_tags(x):\n",
    "    return x        \n",
    "\n",
    "def convert_to_list_dict(path, file):\n",
    "    path = path.format(file)\n",
    "    with open(path, encoding=\"UTF-8\") as infile:\n",
    "        lst = []\n",
    "        tokens = list(conllu.parse_incr(infile))\n",
    "        for sent in tokens:\n",
    "                dic = {\n",
    "                \"idx\": sent.metadata[\"sent_id\"],\n",
    "                \"text\": sent.metadata[\"text\"].lower(),\n",
    "                \"tokens\": [token[\"form\"].lower() for token in sent],\n",
    "                \"lemmas\": [token[\"lemma\"] for token in sent],\n",
    "                \"pos_tags\": [token[\"upos\"] for token in sent],\n",
    "                \"ner_tags\": [filter_tags(token[\"misc\"].get(\"name\", \"O\")) for token in sent],\n",
    "            }\n",
    "                lst.append(dic) \n",
    "        print(\"Converting {} to list of dictionaries\\n     {} elements converted..\".format(file, len(lst)))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/dataset.py\n",
    "\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, x_tokens, y_labels, ner_vocab=None):\n",
    "        self.tokens = [[x for x in entry] for entry in x_tokens]\n",
    "        self.ner_labels = [[y for y in entry] for entry in y_labels]\n",
    "\n",
    "        # hard coded ner_vocab to avoid random shuffled instanciation of ordering of ner_vocab\n",
    "        self.ner_vocab = ner_vocab\n",
    "        self.ner_indexer = {i: n for n, i in enumerate(self.ner_vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.tokens[index]\n",
    "        ner_labels = self.ner_labels[index]\n",
    "\n",
    "        x = tokens\n",
    "        y = torch.LongTensor([self.ner_indexer[i] if i in self.ner_vocab\n",
    "                              else self.ner_indexer['@UNK'] for i in ner_labels])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dynamic padding. Takes the longest sentence in batch and pads other sentences to its length (if im not mistaken)\"\"\"\n",
    "# Function borrowed from https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/bert_ner.py\n",
    "\n",
    "def collate_fn(batch, gpu=False):\n",
    "    longest_y = max([y.size(0) for X, y in batch])\n",
    "    x = [X for X, y in batch]\n",
    "    y = torch.stack(\n",
    "        [functional.pad(y, (0, longest_y - y.size(0)), value=-1) for X, y in batch]) #https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
    "    if gpu:\n",
    "        y = y.to(\"cuda\")\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, ner_vocab, model_path=None, freeze=False):\n",
    "        super().__init__()\n",
    "        self._bert = BertModel.from_pretrained(\n",
    "            model_path\n",
    "        )\n",
    "        hidden_size = self._bert.config.hidden_size\n",
    "        self._linear = nn.Linear(hidden_size, len(ner_vocab))\n",
    "\n",
    "        if freeze:\n",
    "            for param in self._bert.parameters():\n",
    "                param.requires_grad = False #Freezing bert layer\n",
    "\n",
    "    def forward(self, batch, mask):\n",
    "        b = self._bert(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        pooler = b.last_hidden_state[:, mask].diagonal().permute(2, 0, 1) #https://pytorch.org/docs/stable/generated/torch.permute.html\n",
    "        return self._linear(pooler)                                     #https://pytorch.org/docs/stable/generated/torch.diagonal.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/bert_ner.py \n",
    "\n",
    "def build_mask(tokenizer, ids):\n",
    "    tok_sents = [tokenizer.convert_ids_to_tokens(i) for i in ids]\n",
    "    mask = []\n",
    "    for sentence in tok_sents:\n",
    "        current = []\n",
    "        for n, token in enumerate(sentence):\n",
    "            if token in tokenizer.all_special_tokens[1:] or token.startswith(\"##\"): # ## masked\n",
    "                continue\n",
    "            else:\n",
    "                current.append(n)\n",
    "        mask.append(current)\n",
    "\n",
    "    mask = tokenizer.pad({\"input_ids\": mask}, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, tokenizer, model, gpu=False):\n",
    "    input_data = tokenizer(\n",
    "        input_data, is_split_into_words=True, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    if gpu:\n",
    "        input_data = input_data.to(\"cuda\")\n",
    "    batch_mask = build_mask(tokenizer, input_data[\"input_ids\"])\n",
    "    y_pred = model(input_data, batch_mask).permute(0, 2, 1).argmax(dim=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test_set, ner_vocab, tokenizer, model):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    test_set = tqdm.tqdm(test_set)\n",
    "    for x, y in test_set:\n",
    "        y_pred = predict(x, tokenizer, model, gpu=gpu)\n",
    "        predicted = [ner_vocab[element] for element in y_pred[0]]\n",
    "        predicted_labels += predicted\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_freeze(freeze, model):\n",
    "    if freeze:\n",
    "        lr=0.001\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        lr = 2e-5\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimiser, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parl_corpus(rootdir_parl_corpus, lower=False):\n",
    "    corpora_normal_cap, corpora_lower, paths = [], [], []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir_parl_corpus):\n",
    "        for file in files:\n",
    "            if \"normalized_token_data.json\" in file:\n",
    "                path = (os.path.join(subdir, file))\n",
    "                paths.append(path)\n",
    "\n",
    "    for corpus, path in enumerate(paths):\n",
    "        corpus = ParlamentaryCorpus(path)\n",
    "        corpus = corpus.load_data()\n",
    "        corpora_normal_cap.append(corpus)\n",
    "        for k, v in corpus.items():\n",
    "            if v == []:\n",
    "                print(k)\n",
    "                print(path)\n",
    "\n",
    "    if lower==True:\n",
    "        for corpus, path in enumerate(paths):\n",
    "            corpus = ParlamentaryCorpus(path)\n",
    "            corpus = corpus.load_data(lower=True)\n",
    "            corpora_lower.append(corpus)\n",
    "\n",
    "\n",
    "    return corpora_normal_cap, corpora_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parl_sentences_only(dictionary_corpus):\n",
    "    corp = []\n",
    "    for diction in dictionary_corpus:\n",
    "        corp += list(diction.values())\n",
    "    return corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Provides dummy y-data so CoNLLDataset class can be used \"\"\"\n",
    "\"\"\" Not elegant but works perfectly fine :)                 \"\"\"\n",
    "\n",
    "def make_dummy_y(test_corpus):\n",
    "    y_dummy_data = []\n",
    "    for sentence in test_corpus:\n",
    "        dummy = []\n",
    "        dummy.extend(len(sentence)*\"O\")\n",
    "        y_dummy_data.append(dummy)\n",
    "    return y_dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_parl_from_iob2(array):\n",
    "    temp_lst = []\n",
    "    for tag in array:\n",
    "        if tag == \"O\":\n",
    "            temp_lst.append(0)\n",
    "        else:\n",
    "            temp_lst.append(1)\n",
    "    return temp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_labels_unlabled_data(unlabeled):\n",
    "    length = 0\n",
    "    gold_labels = []\n",
    "    for sentences in unlabeled:\n",
    "        label = []\n",
    "        length += len(sentences)\n",
    "        for token in sentences:\n",
    "            if token[0].isupper() == True: # Checks if token has captial letter\n",
    "                label.append(1)\n",
    "                \n",
    "            else:\n",
    "                label.append(0) # Adds 0 if token has lower \n",
    "        gold_labels.append(label)\n",
    "    \n",
    "    return gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_binary(set):\n",
    "    binary_labels = []\n",
    "    for label in set:\n",
    "        if label == \"O\":\n",
    "            binary_labels.append(0)\n",
    "        else:\n",
    "            binary_labels.append(1)\n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making list of lists preds, same len as NorNE gold labels\"\"\"\n",
    "\"\"\" test_dataset_ner_labels should be test_dataset.ner_labels \"\"\"\n",
    "\n",
    "def split_list_preds(preds, test_dataset_ner_labels):\n",
    "    split_list_preds = []\n",
    "    start = 0\n",
    "\n",
    "    for sublist in test_dataset_ner_labels:\n",
    "        end = start + len(sublist)\n",
    "        split_list_preds.append(preds[start:end])\n",
    "        start = end\n",
    "    return split_list_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes that it is the same vocab in dev and test\n",
    "from itertools import chain\n",
    "def get_labels(train_split, add_UNK=True):\n",
    "    label_vocab = [set(y[\"ner_tags\"]) for y in train_split]\n",
    "    label_vocab = list(chain(*[d for d in label_vocab]))\n",
    "    label_vocab = list(set(sorted(label_vocab)))\n",
    "    \n",
    "    if add_UNK==True:\n",
    "        label_vocab.append(\"@UNK\")\n",
    "\n",
    "    tmp1, tmp2 = [], []\n",
    "    for label in label_vocab:\n",
    "        if \"-\" in label:\n",
    "            tmp1.append(label)\n",
    "        else:\n",
    "            tmp2.append(label)\n",
    "    tmp1.sort(key=lambda x: (x.split(\"-\")[1], x.split(\"-\")[0]))\n",
    "    tmp1 = [x for x in tmp1]\n",
    "    tmp1.extend(tmp2)\n",
    "    label_vocab = tmp1\n",
    "    return label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Norne Data loading stuff\"\"\"\n",
    "\n",
    "path = \"all_conllu/{0}.conllu\"\n",
    "file_list = [\"no_bokmaal-ud-dev\", \"no_bokmaal-ud-test\", \"no_bokmaal-ud-train\", \"no_nynorsk-ud-dev\", \"no_nynorsk-ud-test\", \"no_nynorsk-ud-train\"]\n",
    "\n",
    "train_split_no = convert_to_list_dict(path, file_list[2])\n",
    "train_split_ny = convert_to_list_dict(path, file_list[5])\n",
    "\n",
    "\n",
    "train_split = train_split_no + train_split_ny\n",
    "ner_vocab = get_labels(train_split)\n",
    "print(\"Success!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LOADING NPSC SAMPLES \"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('200_annoterte_setninger.csv', newline='', encoding=\"UTF-8\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    left_col = []\n",
    "    right_col = []\n",
    "    for row in csvreader:\n",
    "        left_col.append(row[0])\n",
    "        right_col.append(row[1])\n",
    "\n",
    "x, y = [], []\n",
    "\n",
    "with open(\"parl_annotation_comma_fixed 100 sents.csv\", \"r\", encoding=\"UTF-8\") as f_input:\n",
    "    reader = csv.reader(f_input, delimiter=\",\")\n",
    "    for i, row in enumerate(reader):\n",
    "        if i%2:\n",
    "            y.append(row)\n",
    "        else:\n",
    "            x.append(row)\n",
    "\n",
    "\n",
    "def create_sublists(lst):\n",
    "    sublists = []\n",
    "    sublist = []\n",
    "    for element in lst:\n",
    "        if element == '':\n",
    "            if sublist:\n",
    "                sublists.append(sublist)\n",
    "                sublist = []\n",
    "        else:\n",
    "            sublist.append(element)\n",
    "\n",
    "    if sublist:\n",
    "        sublists.append(sublist)\n",
    "\n",
    "    return sublists\n",
    "\n",
    "y200 = create_sublists(right_col)\n",
    "x200 = create_sublists(left_col)\n",
    "\n",
    "\n",
    "def lower_list_of_lists(lists):\n",
    "    return [[word.lower() for word in sublist] for sublist in lists]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=lower_list_of_lists(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in y for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CoNLLDataset(test, y, ner_vocab)\n",
    "test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=partial(collate_fn, gpu=gpu))\n",
    "\n",
    "\"\"\" Gold labels for test_dataset\"\"\"\n",
    "gold_labels = []\n",
    "for sentence_labels in test_dataset.ner_labels:\n",
    "    for label in sentence_labels:\n",
    "        gold_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_misclassification (gold_sentences_samples_splits, sample_sentence_preds_BINARY_splits, sample_sentences_lower, sample_sentences):\n",
    "    false_negatives = {}\n",
    "    false_positives = {}\n",
    "    for i, (gold, pred) in enumerate(zip(gold_sentences_samples_splits, sample_sentence_preds_BINARY_splits)):\n",
    "        if gold != pred:\n",
    "            for word_index, (word_g, word_p) in enumerate(zip(gold,pred)):\n",
    "                if word_g != word_p:\n",
    "                    if word_p == 0:\n",
    "                            if sample_sentences_lower[i][word_index] not in false_negatives.keys():\n",
    "                                sentence_list = [sample_sentences[i]]\n",
    "                                false_negatives[sample_sentences_lower[i][word_index]] = {\n",
    "                                    \"Count\": 1,\n",
    "                                    \"Sentence\": sentence_list\n",
    "                                    }\n",
    "                            \n",
    "                            elif sample_sentences_lower[i][word_index] in false_negatives.keys():\n",
    "                                count = false_negatives[sample_sentences_lower[i][word_index]][\"Count\"]\n",
    "                                count += 1\n",
    "                                false_negatives[sample_sentences_lower[i][word_index]][\"Count\"] = count\n",
    "                                false_negatives[sample_sentences_lower[i][word_index]][\"Sentence\"].append(sample_sentences[i])\n",
    "\n",
    "\n",
    "\n",
    "                    elif word_p == 1:\n",
    "                            if sample_sentences_lower[i][word_index] not in false_positives.keys():\n",
    "                                sentence_list = [sample_sentences[i]]\n",
    "                                false_positives[sample_sentences_lower[i][word_index]] = {\n",
    "                                    \"Count\": 1,\n",
    "                                    \"Sentence\": sentence_list\n",
    "                                    }\n",
    "                            \n",
    "                            elif sample_sentences_lower[i][word_index] in false_positives.keys():\n",
    "                                count = false_positives[sample_sentences_lower[i][word_index]][\"Count\"]\n",
    "                                count += 1\n",
    "                                false_positives[sample_sentences_lower[i][word_index]][\"Count\"] = count\n",
    "                                false_positives[sample_sentences_lower[i][word_index]][\"Sentence\"].append(sample_sentences[i])\n",
    "\n",
    "    return false_negatives, false_positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# relative paths only works half of the time on my windows machine :(\n",
    "model_types = {\n",
    "    \"ltgoslo/norbert\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\norBERT\",\n",
    "    \"ltgoslo/norbert2\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\norBERT2\",\n",
    "    \"NbAiLab/nb-bert-base\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\NB-BERT\",\n",
    "    \"saattrupdan/nbailab-base-ner-scandi\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\scandi_ner\",\n",
    "    r\"C:\\Users\\Aarne\\bert-base-multilingual-cased\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\mBERT\",\n",
    "}\n",
    "\n",
    "for model_type, trained_models_path in model_types.items():\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_type, do_basic_tokenize=False)\n",
    "    for subdir, dirs, files in os.walk(trained_models_path):\n",
    "        for file in files:\n",
    "            if \".json\" not in file:\n",
    "                bert_model = Bert(ner_vocab, model_type, freeze=False).to(device)\n",
    "                bert_train_path = os.path.join(subdir, file)\n",
    "                bert_model.load_state_dict(torch.load(bert_train_path))\n",
    "                \n",
    "                # Writing stuff for norne data\n",
    "\n",
    "                y_pred_norne = predict_test(test_loader, ner_vocab, tokenizer, bert_model)\n",
    "\n",
    "                # Transformations\n",
    "                gold_binary = transform_to_binary(gold_labels)\n",
    "                preds_binary = transform_to_binary(y_pred_norne)\n",
    "                split_preds = split_list_preds(y_pred_norne, test_dataset.ner_labels)\n",
    "\n",
    "                # classification reports\n",
    "                norne_cr_df_all_labels = classification_report(gold_labels, y_pred_norne, labels = ner_vocab[:-2], digits=5, output_dict=True)\n",
    "                norne_cr_IOB2 = cr(test_dataset.ner_labels, split_preds, mode='strict', scheme=IOB2, digits=4, output_dict=True)\n",
    "                norne_cr_binary = classification_report(gold_binary, preds_binary, digits=5, output_dict=True)\n",
    "\n",
    "                norne_cr_df_all_labels = pd.DataFrame(norne_cr_df_all_labels).transpose()\n",
    "                norne_cr_df_IOB2 = pd.DataFrame(norne_cr_IOB2).transpose()\n",
    "                norne_cr_df_binary = pd.DataFrame(norne_cr_binary).transpose()\n",
    "\n",
    "\n",
    "                # confusion matrixes\n",
    "                norne_cf_matrix_no_O = confusion_matrix(gold_labels, y_pred_norne, labels = ner_vocab[:-2])\n",
    "                norne_cf_matrix_O = confusion_matrix(gold_labels, y_pred_norne, labels = ner_vocab[:-1])\n",
    "                norne_cr_matrix_binary = confusion_matrix(gold_binary, preds_binary)\n",
    "\n",
    "                norne_cf_df_no_O = pd.DataFrame(norne_cf_matrix_no_O)\n",
    "                norne_cf_df_O = pd.DataFrame(norne_cf_matrix_O)\n",
    "                norne_cf_df_binary = pd.DataFrame(norne_cr_matrix_binary)\n",
    "\n",
    "                norne_cf_df_no_O.columns = ner_vocab[:-2]\n",
    "                norne_cf_df_no_O.index = ner_vocab[:-2]\n",
    "                norne_cf_df_O.columns = ner_vocab[:-1]\n",
    "                norne_cf_df_O.index = ner_vocab[:-1]\n",
    "\n",
    "                if \"/\" in model_type:\n",
    "                    model_type_name = model_type.rsplit('/',1)[1]\n",
    "                else:\n",
    "                    model_type_name = model_type\n",
    "                    \n",
    "                    \n",
    "                file_name_cr_all_labels = f\"NSPC_SAMPLES_CR_all_labels_{model_type_name}.csv\" \n",
    "                file_name_cr_IOB2 = f\"NSPC_SAMPLES_CR_IOB2_{model_type_name}.csv\" \n",
    "                file_name_cr_binary = f\"NSPC_SAMPLES_CR_binary_{model_type_name}.csv\" \n",
    "\n",
    "\n",
    "                file_name_cf_no_O = f\"NSPC_SAMPLES_CF_MATRIX_NO_O_{model_type_name}.csv\"\n",
    "                file_name_cf_O = f\"NSPC_SAMPLES_CF_MATRIX_O_{model_type_name}.csv\"\n",
    "                file_name_cf_binary = f\"NSPC_SAMPLES_CF_MATRIX_binary_{model_type_name}.csv\"\n",
    "\n",
    "                current_directory = os.getcwd()\n",
    "                final_directory = os.path.join(current_directory, rf\"{model_type_name}\")\n",
    "                if not os.path.exists(final_directory):\n",
    "                    os.makedirs(final_directory)\n",
    "\n",
    "                # CR REPROTS\n",
    "                with open(f\"{final_directory}\\{file_name_cr_all_labels}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"{file_name_cr_all_labels[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cr_IOB2}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"{file_name_cr_IOB2[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cr_binary}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"{file_name_cr_binary[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                # matrixes\n",
    "                with open(f\"{final_directory}\\{file_name_cf_no_O}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"{file_name_cf_no_O[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cf_O}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"{file_name_cf_O[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cf_binary}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"{file_name_cf_binary[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "                # class report\n",
    "                norne_cr_df_all_labels.to_csv(f\"{final_directory}\\{file_name_cr_all_labels}\", mode=\"a\")\n",
    "                norne_cr_df_IOB2.to_csv(f\"{final_directory}\\{file_name_cr_IOB2}\", mode=\"a\")\n",
    "                norne_cr_df_binary.to_csv(f\"{final_directory}\\{file_name_cr_binary}\", mode=\"a\")\n",
    "\n",
    "                #confusion matrix\n",
    "                norne_cf_df_no_O.to_csv(f\"{final_directory}\\{file_name_cf_no_O}\", mode=\"a\")\n",
    "                norne_cf_df_O.to_csv(f\"{final_directory}\\{file_name_cf_O}\", mode=\"a\")\n",
    "                norne_cf_df_binary.to_csv(f\"{final_directory}\\{file_name_cf_binary}\", mode=\"a\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_NPSC_for_NSRFpp.bmes\", 'w', encoding='utf-8') as f:\n",
    "        for sentence, label in zip(test, y):\n",
    "            for word, label in zip(sentence, label):\n",
    "                f.write(f\"{word} {label} \\n\")\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e5f1d1d80de0e037045f44f922af48a35ae55538dc259ba83b72568cdb122e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
