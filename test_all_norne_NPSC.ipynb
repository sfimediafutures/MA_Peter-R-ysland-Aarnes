{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import conllu\n",
    "import tqdm\n",
    "import torch\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional\n",
    "from functools import partial\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "from get_parl_corpus_token_data import ParlamentaryCorpus\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import classification_report as cr\n",
    "from seqeval.metrics import performance_measure\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import performance_measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Could tidy up this one to just use device instead of calling gpu=true etc.\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    gpu = True\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    gpu = False\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conllu stuff\n",
    "def filter_tags(x):\n",
    "    return x        \n",
    "\n",
    "def convert_to_list_dict(path, file):\n",
    "    path = path.format(file)\n",
    "    with open(path, encoding=\"UTF-8\") as infile:\n",
    "        lst = []\n",
    "        tokens = list(conllu.parse_incr(infile))\n",
    "        for sent in tokens:\n",
    "                dic = {\n",
    "                \"idx\": sent.metadata[\"sent_id\"],\n",
    "                \"text\": sent.metadata[\"text\"].lower(),\n",
    "                \"tokens\": [token[\"form\"].lower() for token in sent],\n",
    "                \"lemmas\": [token[\"lemma\"] for token in sent],\n",
    "                \"pos_tags\": [token[\"upos\"] for token in sent],\n",
    "                \"ner_tags\": [filter_tags(token[\"misc\"].get(\"name\", \"O\")) for token in sent],\n",
    "            }\n",
    "                lst.append(dic) \n",
    "        print(\"Converting {} to list of dictionaries\\n     {} elements converted..\".format(file, len(lst)))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/dataset.py\n",
    "\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, x_tokens, y_labels, ner_vocab=None):\n",
    "        self.tokens = [[x for x in entry] for entry in x_tokens]\n",
    "        self.ner_labels = [[y for y in entry] for entry in y_labels]\n",
    "\n",
    "        # hard coded ner_vocab to avoid random shuffled instanciation of ordering of ner_vocab\n",
    "        self.ner_vocab = ner_vocab\n",
    "        self.ner_indexer = {i: n for n, i in enumerate(self.ner_vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.tokens[index]\n",
    "        ner_labels = self.ner_labels[index]\n",
    "\n",
    "        x = tokens\n",
    "        y = torch.LongTensor([self.ner_indexer[i] if i in self.ner_vocab\n",
    "                              else self.ner_indexer['@UNK'] for i in ner_labels])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dynamic padding. Takes the longest sentence in batch and pads other sentences to its length (if im not mistaken)\"\"\"\n",
    "# Function borrowed from https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/bert_ner.py\n",
    "\n",
    "def collate_fn(batch, gpu=False):\n",
    "    longest_y = max([y.size(0) for X, y in batch])\n",
    "    x = [X for X, y in batch]\n",
    "    y = torch.stack(\n",
    "        [functional.pad(y, (0, longest_y - y.size(0)), value=-1) for X, y in batch]) #https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
    "    if gpu:\n",
    "        y = y.to(\"cuda\")\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, ner_vocab, model_path=None, freeze=False):\n",
    "        super().__init__()\n",
    "        self._bert = BertModel.from_pretrained(\n",
    "            model_path\n",
    "        )\n",
    "        hidden_size = self._bert.config.hidden_size\n",
    "        self._linear = nn.Linear(hidden_size, len(ner_vocab))\n",
    "\n",
    "        if freeze:\n",
    "            for param in self._bert.parameters():\n",
    "                param.requires_grad = False #Freezing bert layer\n",
    "\n",
    "    def forward(self, batch, mask):\n",
    "        b = self._bert(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        pooler = b.last_hidden_state[:, mask].diagonal().permute(2, 0, 1) #https://pytorch.org/docs/stable/generated/torch.permute.html\n",
    "        return self._linear(pooler)                                     #https://pytorch.org/docs/stable/generated/torch.diagonal.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/bert_ner.py \n",
    "\n",
    "def build_mask(tokenizer, ids):\n",
    "    tok_sents = [tokenizer.convert_ids_to_tokens(i) for i in ids]\n",
    "    mask = []\n",
    "    for sentence in tok_sents:\n",
    "        current = []\n",
    "        for n, token in enumerate(sentence):\n",
    "            if token in tokenizer.all_special_tokens[1:] or token.startswith(\"##\"): # ## masked\n",
    "                continue\n",
    "            else:\n",
    "                current.append(n)\n",
    "        mask.append(current)\n",
    "\n",
    "    mask = tokenizer.pad({\"input_ids\": mask}, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, tokenizer, model, gpu=False):\n",
    "    input_data = tokenizer(\n",
    "        input_data, is_split_into_words=True, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    if gpu:\n",
    "        input_data = input_data.to(\"cuda\")\n",
    "    batch_mask = build_mask(tokenizer, input_data[\"input_ids\"])\n",
    "    y_pred = model(input_data, batch_mask).permute(0, 2, 1).argmax(dim=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test_set, ner_vocab, tokenizer, model):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    test_set = tqdm.tqdm(test_set)\n",
    "    for x, y in test_set:\n",
    "        y_pred = predict(x, tokenizer, model, gpu=gpu)\n",
    "        predicted = [ner_vocab[element] for element in y_pred[0]]\n",
    "        predicted_labels += predicted\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_freeze(freeze, model):\n",
    "    if freeze:\n",
    "        lr=0.001\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        lr = 2e-5\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimiser, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parl_corpus(rootdir_parl_corpus, lower=False):\n",
    "    corpora_normal_cap, corpora_lower, paths = [], [], []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir_parl_corpus):\n",
    "        for file in files:\n",
    "            if \"normalized_token_data.json\" in file:\n",
    "                path = (os.path.join(subdir, file))\n",
    "                paths.append(path)\n",
    "\n",
    "    for corpus, path in enumerate(paths):\n",
    "        corpus = ParlamentaryCorpus(path)\n",
    "        corpus = corpus.load_data()\n",
    "        corpora_normal_cap.append(corpus)\n",
    "        for k, v in corpus.items():\n",
    "            if v == []:\n",
    "                print(k)\n",
    "                print(path)\n",
    "\n",
    "    if lower==True:\n",
    "        for corpus, path in enumerate(paths):\n",
    "            corpus = ParlamentaryCorpus(path)\n",
    "            corpus = corpus.load_data(lower=True)\n",
    "            corpora_lower.append(corpus)\n",
    "\n",
    "\n",
    "    return corpora_normal_cap, corpora_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parl_sentences_only(dictionary_corpus):\n",
    "    corp = []\n",
    "    for diction in dictionary_corpus:\n",
    "        corp += list(diction.values())\n",
    "    return corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Provides dummy y-data so CoNLLDataset class can be used \"\"\"\n",
    "\"\"\" Not elegant but works perfectly fine :)                 \"\"\"\n",
    "\n",
    "def make_dummy_y(test_corpus):\n",
    "    y_dummy_data = []\n",
    "    for sentence in test_corpus:\n",
    "        dummy = []\n",
    "        dummy.extend(len(sentence)*\"O\")\n",
    "        y_dummy_data.append(dummy)\n",
    "    return y_dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_parl_from_iob2(array):\n",
    "    temp_lst = []\n",
    "    for tag in array:\n",
    "        if tag == \"O\":\n",
    "            temp_lst.append(0)\n",
    "        else:\n",
    "            temp_lst.append(1)\n",
    "    return temp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_labels_unlabled_data(unlabeled):\n",
    "    length = 0\n",
    "    gold_labels = []\n",
    "    for sentences in unlabeled:\n",
    "        label = []\n",
    "        length += len(sentences)\n",
    "        for token in sentences:\n",
    "            if token[0].isupper() == True: # Checks if token has captial letter\n",
    "                label.append(1)\n",
    "                \n",
    "            else:\n",
    "                label.append(0) # Adds 0 if token has lower \n",
    "        gold_labels.append(label)\n",
    "    \n",
    "    return gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_binary(set):\n",
    "    binary_labels = []\n",
    "    for label in set:\n",
    "        if label == \"O\":\n",
    "            binary_labels.append(0)\n",
    "        else:\n",
    "            binary_labels.append(1)\n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making list of lists preds, same len as NorNE gold labels\"\"\"\n",
    "\"\"\" test_dataset_ner_labels should be test_dataset.ner_labels \"\"\"\n",
    "\n",
    "def split_list_preds(preds, test_dataset_ner_labels):\n",
    "    split_list_preds = []\n",
    "    start = 0\n",
    "\n",
    "    for sublist in test_dataset_ner_labels:\n",
    "        end = start + len(sublist)\n",
    "        split_list_preds.append(preds[start:end])\n",
    "        start = end\n",
    "    return split_list_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes that it is the same vocab in dev and test\n",
    "from itertools import chain\n",
    "def get_labels(train_split, add_UNK=True):\n",
    "    label_vocab = [set(y[\"ner_tags\"]) for y in train_split]\n",
    "    label_vocab = list(chain(*[d for d in label_vocab]))\n",
    "    label_vocab = list(set(sorted(label_vocab)))\n",
    "    \n",
    "    if add_UNK==True:\n",
    "        label_vocab.append(\"@UNK\")\n",
    "\n",
    "    tmp1, tmp2 = [], []\n",
    "    for label in label_vocab:\n",
    "        if \"-\" in label:\n",
    "            tmp1.append(label)\n",
    "        else:\n",
    "            tmp2.append(label)\n",
    "    tmp1.sort(key=lambda x: (x.split(\"-\")[1], x.split(\"-\")[0]))\n",
    "    tmp1 = [x for x in tmp1]\n",
    "    tmp1.extend(tmp2)\n",
    "    label_vocab = tmp1\n",
    "    return label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting no_bokmaal-ud-test to list of dictionaries\n",
      "     1939 elements converted..\n",
      "Converting no_bokmaal-ud-train to list of dictionaries\n",
      "     15696 elements converted..\n",
      "Converting no_nynorsk-ud-test to list of dictionaries\n",
      "     1511 elements converted..\n",
      "Converting no_nynorsk-ud-train to list of dictionaries\n",
      "     14174 elements converted..\n",
      "Combining test set..\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Norne Data loading stuff\"\"\"\n",
    "\n",
    "path = \"all_conllu/{0}.conllu\"\n",
    "file_list = [\"no_bokmaal-ud-dev\", \"no_bokmaal-ud-test\", \"no_bokmaal-ud-train\", \"no_nynorsk-ud-dev\", \"no_nynorsk-ud-test\", \"no_nynorsk-ud-train\"]\n",
    "\n",
    "\n",
    "test_split_no = convert_to_list_dict(path, file_list[1])\n",
    "train_split_no = convert_to_list_dict(path, file_list[2])\n",
    "\n",
    "\n",
    "test_split_ny = convert_to_list_dict(path, file_list[4])\n",
    "train_split_ny = convert_to_list_dict(path, file_list[5])\n",
    "\n",
    "\n",
    "\n",
    "train_split = train_split_no + train_split_ny\n",
    "ner_vocab = get_labels(train_split)\n",
    "\n",
    "\n",
    "print(\"Combining test set..\")\n",
    "test_split = test_split_no + test_split_ny\n",
    "print(\"Success!\")\n",
    "\n",
    "x_test_tokens = [x[\"tokens\"] for x in test_split]\n",
    "y_test_labels = [y[\"ner_tags\"] for y in test_split]\n",
    "test_dataset = CoNLLDataset(x_test_tokens, y_test_labels, ner_vocab)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=partial(collate_fn, gpu=gpu))\n",
    "\n",
    "\"\"\" Gold labels for NorNE test_dataset\"\"\"\n",
    "gold_labels = []\n",
    "for sentence_labels in test_dataset.ner_labels:\n",
    "    for label in sentence_labels:\n",
    "        gold_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parliamentary corpus data loading stuff\"\"\"\n",
    "\n",
    "rootdir=r\"C:\\Users\\Aarne\\Desktop\\Ferdig_code_folder\\parl_corpus_full\"\n",
    "corpora_normal_cap, corpora_lower = load_parl_corpus(rootdir, lower=True)\n",
    "gold_corp = parl_sentences_only(corpora_normal_cap)\n",
    "test_parl = parl_sentences_only(corpora_lower)\n",
    "\n",
    "_gold_labels_parl = gold_labels_unlabled_data(gold_corp)\n",
    "\n",
    "dummy_y_parl = make_dummy_y(test_parl)\n",
    "parl_lower_corpus = CoNLLDataset(test_parl, dummy_y_parl, ner_vocab)\n",
    "\n",
    "parl_loader = DataLoader(\n",
    "        parl_lower_corpus, batch_size=1, shuffle=False, collate_fn=partial(collate_fn, gpu=gpu))\n",
    "\n",
    "gold_labels_parl = [item for sublist  in _gold_labels_parl for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_misclassification (gold_sentences_samples_splits, sample_sentence_preds_BINARY_splits, sample_sentences_lower, sample_sentences):\n",
    "    false_negatives = {}\n",
    "    false_positives = {}\n",
    "    for i, (gold, pred) in enumerate(zip(gold_sentences_samples_splits, sample_sentence_preds_BINARY_splits)):\n",
    "        if gold != pred:\n",
    "            for word_index, (word_g, word_p) in enumerate(zip(gold,pred)):\n",
    "                if word_g != word_p:\n",
    "                    if word_p == 0:\n",
    "                            if sample_sentences_lower[i][word_index] not in false_negatives.keys():\n",
    "                                sentence_list = [sample_sentences[i]]\n",
    "                                false_negatives[sample_sentences_lower[i][word_index]] = {\n",
    "                                    \"Count\": 1,\n",
    "                                    \"Sentence\": sentence_list\n",
    "                                    }\n",
    "                            \n",
    "                            elif sample_sentences_lower[i][word_index] in false_negatives.keys():\n",
    "                                count = false_negatives[sample_sentences_lower[i][word_index]][\"Count\"]\n",
    "                                count += 1\n",
    "                                false_negatives[sample_sentences_lower[i][word_index]][\"Count\"] = count\n",
    "                                false_negatives[sample_sentences_lower[i][word_index]][\"Sentence\"].append(sample_sentences[i])\n",
    "\n",
    "\n",
    "\n",
    "                    elif word_p == 1:\n",
    "                            if sample_sentences_lower[i][word_index] not in false_positives.keys():\n",
    "                                sentence_list = [sample_sentences[i]]\n",
    "                                false_positives[sample_sentences_lower[i][word_index]] = {\n",
    "                                    \"Count\": 1,\n",
    "                                    \"Sentence\": sentence_list\n",
    "                                    }\n",
    "                            \n",
    "                            elif sample_sentences_lower[i][word_index] in false_positives.keys():\n",
    "                                count = false_positives[sample_sentences_lower[i][word_index]][\"Count\"]\n",
    "                                count += 1\n",
    "                                false_positives[sample_sentences_lower[i][word_index]][\"Count\"] = count\n",
    "                                false_positives[sample_sentences_lower[i][word_index]][\"Sentence\"].append(sample_sentences[i])\n",
    "\n",
    "    return false_negatives, false_positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 22%|██▏       | 756/3450 [00:09<00:32, 82.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m bert_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(bert_train_path))\n\u001b[0;32m     19\u001b[0m \u001b[39m# Writing stuff for norne data\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m y_pred_norne \u001b[39m=\u001b[39m predict_test(test_loader, ner_vocab, tokenizer, bert_model)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Transformations\u001b[39;00m\n\u001b[0;32m     24\u001b[0m gold_binary \u001b[39m=\u001b[39m transform_to_binary(gold_labels)\n",
      "Cell \u001b[1;32mIn [10], line 6\u001b[0m, in \u001b[0;36mpredict_test\u001b[1;34m(test_set, ner_vocab, tokenizer, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m test_set \u001b[39m=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(test_set)\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m test_set:\n\u001b[1;32m----> 6\u001b[0m     y_pred \u001b[39m=\u001b[39m predict(x, tokenizer, model, gpu\u001b[39m=\u001b[39;49mgpu)\n\u001b[0;32m      7\u001b[0m     predicted \u001b[39m=\u001b[39m [ner_vocab[element] \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m y_pred[\u001b[39m0\u001b[39m]]\n\u001b[0;32m      8\u001b[0m     predicted_labels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m predicted\n",
      "Cell \u001b[1;32mIn [9], line 8\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(input_data, tokenizer, model, gpu)\u001b[0m\n\u001b[0;32m      6\u001b[0m     input_data \u001b[39m=\u001b[39m input_data\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m batch_mask \u001b[39m=\u001b[39m build_mask(tokenizer, input_data[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m y_pred \u001b[39m=\u001b[39m model(input_data, batch_mask)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [7], line 15\u001b[0m, in \u001b[0;36mBert.forward\u001b[1;34m(self, batch, mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, batch, mask):\n\u001b[1;32m---> 15\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bert(\n\u001b[0;32m     16\u001b[0m         input_ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     pooler \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mlast_hidden_state[:, mask]\u001b[39m.\u001b[39mdiagonal()\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m#https://pytorch.org/docs/stable/generated/torch.permute.html\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_linear(pooler)\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# relative paths only works half of the time on my windows machine :(\n",
    "model_types = {\n",
    "    \"bert-base-multilingual-cased\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\mBERT\",\n",
    "    \"ltgoslo/norbert\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\norBERT\",\n",
    "    \"ltgoslo/norbert2\": r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\MSTR-PY\\TRAIN ALL BERTS\\trained\\NB-BERT\",\n",
    "    \"NbAiLab/nb-bert-base\": r\"C:\\Users\\Aarne\\Desktop\\Ferdig_code_folder\\BERT models\\NB-BERT\\trained models nb-bert\",\n",
    "    \"saattrupdan/nbailab-base-ner-scandi\": r\"C:\\Users\\Aarne\\Desktop\\Ferdig_code_folder\\BERT models\\scandi-bert\\trained models scandi_bert\"\n",
    "}\n",
    "\n",
    "for model_type, trained_models_path in model_types.items():\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_type, do_basic_tokenize=False)\n",
    "    for subdir, dirs, files in os.walk(trained_models_path):\n",
    "        for file in files:\n",
    "            if \".json\" not in file:\n",
    "                bert_model = Bert(ner_vocab, model_type, freeze=False).to(device)\n",
    "                bert_train_path = os.path.join(subdir, file)\n",
    "                bert_model.load_state_dict(torch.load(bert_train_path))\n",
    "                \n",
    "                # Writing stuff for norne data\n",
    "\n",
    "                y_pred_norne = predict_test(test_loader, ner_vocab, tokenizer, bert_model)\n",
    "\n",
    "                # Transformations\n",
    "                gold_binary = transform_to_binary(gold_labels)\n",
    "                preds_binary = transform_to_binary(y_pred_norne)\n",
    "                split_preds = split_list_preds(y_pred_norne, test_dataset.ner_labels)\n",
    "\n",
    "                # classification reports\n",
    "                norne_cr_df_all_labels = classification_report(gold_labels, y_pred_norne, labels = ner_vocab[:-2], digits=5, output_dict=True)\n",
    "                norne_cr_IOB2 = cr(test_dataset.ner_labels, split_preds, mode='strict', scheme=IOB2, digits=4, output_dict=True)\n",
    "                norne_cr_binary = classification_report(gold_binary, preds_binary, digits=5, output_dict=True)\n",
    "\n",
    "                norne_cr_df_all_labels = pd.DataFrame(norne_cr_df_all_labels).transpose()\n",
    "                norne_cr_df_IOB2 = pd.DataFrame(norne_cr_IOB2).transpose()\n",
    "                norne_cr_df_binary = pd.DataFrame(norne_cr_binary).transpose()\n",
    "\n",
    "\n",
    "                # confusion matrixes\n",
    "                norne_cf_matrix_no_O = confusion_matrix(gold_labels, y_pred_norne, labels = ner_vocab[:-2])\n",
    "                norne_cf_matrix_O = confusion_matrix(gold_labels, y_pred_norne, labels = ner_vocab[:-1])\n",
    "                norne_cr_matrix_binary = confusion_matrix(gold_binary, preds_binary)\n",
    "\n",
    "                norne_cf_df_no_O = pd.DataFrame(norne_cf_matrix_no_O)\n",
    "                norne_cf_df_O = pd.DataFrame(norne_cf_matrix_O)\n",
    "                norne_cf_df_binary = pd.DataFrame(norne_cr_matrix_binary)\n",
    "\n",
    "                norne_cf_df_no_O.columns = ner_vocab[:-2]\n",
    "                norne_cf_df_no_O.index = ner_vocab[:-2]\n",
    "                norne_cf_df_O.columns = ner_vocab[:-1]\n",
    "                norne_cf_df_O.index = ner_vocab[:-1]\n",
    "\n",
    "                if \"/\" in model_type:\n",
    "                    model_type_name = model_type.rsplit('/',1)[1]\n",
    "                else:\n",
    "                    model_type_name = model_type\n",
    "                    \n",
    "                    \n",
    "                file_name_cr_all_labels = f\"norne_CR_all_labels_{model_type_name}.csv\" \n",
    "                file_name_cr_IOB2 = f\"norne_CR_IOB2_{model_type_name}.csv\" \n",
    "                file_name_cr_binary = f\"norne_CR_binary_{model_type_name}.csv\" \n",
    "\n",
    "\n",
    "                file_name_cf_no_O = f\"norne_CF_MATRIX_NO_O_{model_type_name}.csv\"\n",
    "                file_name_cf_O = f\"norne_CF_MATRIX_O_{model_type_name}.csv\"\n",
    "                file_name_cf_binary = f\"norne_CF_MATRIX_binary_{model_type_name}.csv\"\n",
    "\n",
    "                current_directory = os.getcwd()\n",
    "                final_directory = os.path.join(current_directory, rf\"{model_type_name}\")\n",
    "                if not os.path.exists(final_directory):\n",
    "                    os.makedirs(final_directory)\n",
    "\n",
    "                # CR REPROTS\n",
    "                with open(f\"{final_directory}\\{file_name_cr_all_labels}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"NorNE_{file_name_cr_all_labels[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cr_IOB2}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"NorNE_{file_name_cr_IOB2[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cr_binary}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"NorNE_{file_name_cr_binary[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                # matrixes\n",
    "                with open(f\"{final_directory}\\{file_name_cf_no_O}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"NorNE_{file_name_cf_no_O[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cf_O}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"NorNE_{file_name_cf_O[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cf_binary}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"NorNE_{file_name_cf_binary[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "                # class report\n",
    "                norne_cr_df_all_labels.to_csv(f\"{final_directory}\\{file_name_cr_all_labels}\", mode=\"a\")\n",
    "                norne_cr_df_IOB2.to_csv(f\"{final_directory}\\{file_name_cr_IOB2}\", mode=\"a\")\n",
    "                norne_cr_df_binary.to_csv(f\"{final_directory}\\{file_name_cr_binary}\", mode=\"a\")\n",
    "\n",
    "                #confusion matrix\n",
    "                norne_cf_df_no_O.to_csv(f\"{final_directory}\\{file_name_cf_no_O}\", mode=\"a\")\n",
    "                norne_cf_df_O.to_csv(f\"{final_directory}\\{file_name_cf_O}\", mode=\"a\")\n",
    "                norne_cf_df_binary.to_csv(f\"{final_directory}\\{file_name_cf_binary}\", mode=\"a\")\n",
    "\n",
    "\n",
    "                # Parl test \n",
    "                y_pred_parl = predict_test(parl_loader, ner_vocab, tokenizer, bert_model)\n",
    "                y_pred_parl_binary = binary_parl_from_iob2(y_pred_parl)\n",
    "\n",
    "                # for json\n",
    "                preds_BINARY_splits = split_list_preds(y_pred_parl_binary, dummy_y_parl)\n",
    "\n",
    "                parl_cr = classification_report(gold_labels_parl, y_pred_parl_binary, digits=5, output_dict=True)\n",
    "                parl_cf_matrix = confusion_matrix(gold_labels_parl, y_pred_parl_binary)\n",
    "\n",
    "                parl_cr_df = pd.DataFrame(parl_cr).transpose()\n",
    "                parl_cf_df = pd.DataFrame(parl_cf_matrix)\n",
    "\n",
    "                parl_cf_df.columns = [0, 1]\n",
    "                parl_cf_df.index = [0, 1]\n",
    "\n",
    "\n",
    "                file_name_cr = f\"parl_CR_{model_type_name}.csv\" \n",
    "                file_name_cf = f\"parl_CF_MATRIX_{model_type_name}.csv\"\n",
    "\n",
    "                current_directory = os.getcwd()\n",
    "                final_directory = os.path.join(current_directory, rf\"{model_type_name}\")\n",
    "                if not os.path.exists(final_directory):\n",
    "                    os.makedirs(final_directory)\n",
    "\n",
    "\n",
    "                with open(f\"{final_directory}\\{file_name_cr}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"parl_{file_name_cr[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "                with open(f\"{final_directory}\\{file_name_cf}\", \"a\") as f:\n",
    "                    f.write(\"\\n\")\n",
    "                    f.write(f\"parl_{file_name_cf[3:-4]}_seed_{bert_train_path.rsplit('_')[-1]}\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "                parl_cr_df.to_csv(f\"{final_directory}\\{file_name_cr}\", mode=\"a\")\n",
    "                parl_cf_df.to_csv(f\"{final_directory}\\{file_name_cf}\", mode=\"a\")\n",
    "\n",
    "\n",
    "                # Jsons with frequent misclassifications\n",
    "                fn, fp = count_misclassification(_gold_labels_parl, preds_BINARY_splits, test_parl, gold_corp)\n",
    "\n",
    "                file_name_json_FN = f\"false_negatives_seed_{bert_train_path.rsplit('_')[-1]}.json\"\n",
    "                file_name_json_FP = f\"false_positives_seed_{bert_train_path.rsplit('_')[-1]}.json\"\n",
    "\n",
    "                with open(f\"{final_directory}{file_name_json_FN}\", \"w\", encoding=\"utf-8\") as write:\n",
    "                    json.dump(fn, write, indent=2, ensure_ascii= False)\n",
    "                with open(f\"{final_directory}{file_name_json_FP}\", \"w\", encoding=\"utf-8\") as write:\n",
    "                    json.dump(fp, write, indent=2, ensure_ascii=False)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e5f1d1d80de0e037045f44f922af48a35ae55538dc259ba83b72568cdb122e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
