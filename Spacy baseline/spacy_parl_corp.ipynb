{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score, precision_score, recall_score, classification_report\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class ParlamentaryCorpus():\n",
    "\n",
    "    def __init__(self, path): # Takes input normalized_token_file\n",
    "        self.path = path\n",
    "        self.sentence_dict = {}\n",
    "\n",
    "    def load_data(self, lower=False):\n",
    "        sentence_dict = self.sentence_dict\n",
    "        with open(self.path) as infile:\n",
    "            with open(self.path, encoding=\"UTF-8\") as infile:\n",
    "                json_input = json.load(infile)\n",
    "\n",
    "        if lower == False:\n",
    "            for chaos in json_input[\"sentences\"]:\n",
    "                token_list = []\n",
    "                for token in chaos[\"tokens\"]:\n",
    "                    if token[\"special_status\"] != None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if \" \" in token[\"token_text\"]:\n",
    "                            token_list.extend(token[\"token_text\"].split())\n",
    "                        else:\n",
    "                            token_list.append(token[\"token_text\"])\n",
    "                    sentence_dict[chaos[\"sentence_id\"]] = token_list\n",
    "        else:\n",
    "            for chaos in json_input[\"sentences\"]:\n",
    "                token_list = []\n",
    "                for token in chaos[\"tokens\"]:\n",
    "                    if token[\"special_status\"] != None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if \" \" in token[\"token_text\"]:\n",
    "                            token_list.extend(token[\"token_text\"].lower().split())\n",
    "                        else:\n",
    "                            token_list.append(token[\"token_text\"].lower())\n",
    "                    sentence_dict[chaos[\"sentence_id\"]] = token_list\n",
    "                    \n",
    "        return sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parl_corpus(rootdir_parl_corpus, lower=False):\n",
    "    corpora_normal_cap, corpora_lower, paths = [], [], []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir_parl_corpus):\n",
    "        for file in files:\n",
    "            if \"normalized_token_data.json\" in file:\n",
    "                path = (os.path.join(subdir, file))\n",
    "                paths.append(path)\n",
    "\n",
    "    for corpus, path in enumerate(paths):\n",
    "        corpus = ParlamentaryCorpus(path)\n",
    "        corpus = corpus.load_data()\n",
    "        corpora_normal_cap.append(corpus)\n",
    "        for k, v in corpus.items():\n",
    "            if v == []:\n",
    "                print(k)\n",
    "                print(path)\n",
    "\n",
    "    if lower==True:\n",
    "        for corpus, path in enumerate(paths):\n",
    "            corpus = ParlamentaryCorpus(path)\n",
    "            corpus = corpus.load_data(lower=True)\n",
    "            corpora_lower.append(corpus)\n",
    "\n",
    "\n",
    "    return corpora_normal_cap, corpora_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parliamentary corpus data loading stuff\"\"\"\n",
    "nlp = spacy.load(\"nb_core_news_lg\")\n",
    "rootdir=r\"C:\\Users\\Aarne\\Desktop\\Ferdig_code_folder\\parl_corpus_full\"\n",
    "corpora_normal_cap, corpora_lower = load_parl_corpus(rootdir, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [03:20<00:00,  5.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# implementing bindestrek fix\n",
    "\n",
    "pred_sentences = []\n",
    "for diction in tqdm(corpora_lower):\n",
    "    for k, sentence in diction.items():\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        pred_sent = []\n",
    "        for token in doc:\n",
    "            ## Checking if \"-\" (bindestrek) in token. Such as in \"Sør-Trøndelag\"\n",
    "            if token.ent_type_ != \"\": \n",
    "                if \"-\" in token.text:\n",
    "                    s = []\n",
    "                    for i, c in enumerate(token.text):\n",
    "                        if i == 0:\n",
    "                            s.append(c.capitalize())\n",
    "                        elif c == \"-\":\n",
    "                            s.append(\"-\")\n",
    "                        elif token.text[i-1] == \"-\":\n",
    "                            s.append(c.capitalize())\n",
    "                        else:\n",
    "                            s.append(c)\n",
    "                    pred_sent.append(\"\".join(s))\n",
    "                    continue\n",
    "                    \n",
    "                pred_sent.append(token.text.capitalize())\n",
    "            else:\n",
    "                pred_sent.append(token.text)\n",
    "        pred_sentences.append(\" \".join(pred_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_annotations = []\n",
    "for sentence in pred_sentences:\n",
    "    annoation = []\n",
    "    for token in sentence.split():\n",
    "        if token[0].isupper(): # Checks if token has captial letter\n",
    "            annoation.append(1)\n",
    "            \n",
    "        else:\n",
    "            annoation.append(0) # Adds 0 if token has lower \n",
    "    pred_annotations.append((annoation))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [03:19<00:00,  5.13s/it]\n"
     ]
    }
   ],
   "source": [
    "gold_annotations = []\n",
    "docs = []\n",
    "for diction in tqdm(corpora_normal_cap):\n",
    "    for k, word in diction.items():\n",
    "        doc = nlp(\" \".join(word))\n",
    "        docs.append(doc)\n",
    "        annoation = []\n",
    "        for token in doc: \n",
    "            if token.text[0].isupper() == True: # Checks if token has captial letter\n",
    "                annoation.append(1)\n",
    "                \n",
    "            else:\n",
    "                annoation.append(0) # Adds 0 if token has lower \n",
    "        gold_annotations.append((annoation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = []\n",
    "for diction in corpora_lower[:1]:\n",
    "    for k, sentence in diction.items():\n",
    "        doc = nlp(\" \".join(sentence))\n",
    "        pred_sent = []\n",
    "        for token in doc:\n",
    "            ## Checking if \"-\" (bindestrek) in token. Such as in \"Sør-Trøndelag\"\n",
    "            if token.ent_type_ != \"\": \n",
    "                if \"-\" in token.text:\n",
    "                    s = []\n",
    "                    for i, c in enumerate(token.text):\n",
    "                        if i == 0:\n",
    "                            s.append(c.capitalize())\n",
    "                        elif c == \"-\":\n",
    "                            s.append(\"-\")\n",
    "                        elif token.text[i-1] == \"-\":\n",
    "                            s.append(c.capitalize())\n",
    "                        else:\n",
    "                            s.append(c)\n",
    "                    pred_sent.append(\"\".join(s))\n",
    "                    continue\n",
    "                    \n",
    "                pred_sent.append(token.text.capitalize())\n",
    "            else:\n",
    "                pred_sent.append(token.text)\n",
    "        pred_sentences.append(\" \".join(pred_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [item for sublist  in pred_annotations for item in sublist]\n",
    "y_true = [item for sublist  in gold_annotations for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
