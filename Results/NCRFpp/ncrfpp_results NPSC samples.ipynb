{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import classification_report as cr\n",
    "from seqeval.metrics import performance_measure\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import performance_measure\n",
    "import conllu\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score, precision_score, recall_score, classification_report\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\GitHub\\MA_Peter-R-ysland-Aarnes\\Results\\NCRFpp\\prediction_NSCP_samples.txt\", \"r\") as f:\n",
    "    preds_norne = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_norne = [re.sub('\\n', '', line) for line in preds_norne]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavily insipired from:\n",
    "# https://github.com/ltgoslo/NorBERT/blob/main/benchmarking/experiments/dataset.py\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, x_tokens, y_labels, ner_vocab=None):\n",
    "        self.tokens = [[x for x in entry] for entry in x_tokens]\n",
    "        self.ner_labels = [[y for y in entry] for entry in y_labels]\n",
    "\n",
    "        # hard coded ner_vocab to avoid random shuffled instanciation of ordering of ner_vocab\n",
    "        self.ner_vocab = ner_vocab\n",
    "        self.ner_indexer = {i: n for n, i in enumerate(self.ner_vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.tokens[index]\n",
    "        ner_labels = self.ner_labels[index]\n",
    "\n",
    "        x = tokens\n",
    "        y = torch.LongTensor([self.ner_indexer[i] if i in self.ner_vocab\n",
    "                              else self.ner_indexer['@UNK'] for i in ner_labels])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_vocab =     ['B-GPE_LOC', 'I-DRV', 'I-LOC', 'B-PER', 'I-PER', 'B-PROD', \n",
    "                'I-GPE_ORG', 'B-GPE_ORG', 'B-EVT', 'B-DRV', 'I-PROD', 'B-ORG', 'B-MISC',\n",
    "                'I-MISC', 'I-GPE_LOC', 'B-LOC', 'I-ORG', 'I-EVT', 'O', '@UNK']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LOADING NPSC SAMPLES \"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(r'C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\GitHub\\MA_Peter-R-ysland-Aarnes\\IOB NPSC sample\\200_annoterte_setninger.csv', newline='', encoding=\"UTF-8\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',')\n",
    "    left_col = []\n",
    "    right_col = []\n",
    "    for row in csvreader:\n",
    "        left_col.append(row[0])\n",
    "        right_col.append(row[1])\n",
    "\n",
    "x, y = [], []\n",
    "\n",
    "with open(r\"C:\\Users\\Aarne\\OneDrive - University of Bergen\\Dokumenter\\GitHub\\MA_Peter-R-ysland-Aarnes\\IOB NPSC sample\\parl_annotation_comma_fixed 100 sents.csv\", \"r\", encoding=\"UTF-8\") as f_input:\n",
    "    reader = csv.reader(f_input, delimiter=\",\")\n",
    "    for i, row in enumerate(reader):\n",
    "        if i%2:\n",
    "            y.append(row)\n",
    "        else:\n",
    "            x.append(row)\n",
    "\n",
    "\n",
    "def create_sublists(lst):\n",
    "    sublists = []\n",
    "    sublist = []\n",
    "    for element in lst:\n",
    "        if element == '':\n",
    "            if sublist:\n",
    "                sublists.append(sublist)\n",
    "                sublist = []\n",
    "        else:\n",
    "            sublist.append(element)\n",
    "\n",
    "    if sublist:\n",
    "        sublists.append(sublist)\n",
    "\n",
    "    return sublists\n",
    "\n",
    "y200 = create_sublists(right_col)\n",
    "x200 = create_sublists(left_col)\n",
    "x += x200\n",
    "y += y200\n",
    "\n",
    "\n",
    "\n",
    "def lower_list_of_lists(lists):\n",
    "    return [[word.lower() for word in sublist] for sublist in lists]\n",
    "\n",
    "\n",
    "test=lower_list_of_lists(x)\n",
    "test_dataset = CoNLLDataset(test, y, ner_vocab)\n",
    "\n",
    "\"\"\" Gold labels for test_dataset\"\"\"\n",
    "gold_labels = []\n",
    "for sentence_labels in test_dataset.ner_labels:\n",
    "    for label in sentence_labels:\n",
    "        gold_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = []\n",
    "for sentence_labels in test_dataset.ner_labels:\n",
    "    for label in sentence_labels:\n",
    "        gold_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5438"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making list of lists preds, same len as NorNE gold labels\"\"\"\n",
    "\"\"\" test_dataset_ner_labels should be test_dataset.ner_labels \"\"\"\n",
    "\n",
    "def split_list_preds(preds, test_dataset_ner_labels):\n",
    "    split_list_preds = []\n",
    "    start = 0\n",
    "\n",
    "    for sublist in test_dataset_ner_labels:\n",
    "        end = start + len(sublist)\n",
    "        split_list_preds.append(preds[start:end])\n",
    "        start = end\n",
    "    return split_list_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_binary(set):\n",
    "    binary_labels = []\n",
    "    for label in set:\n",
    "        if label == \"O\":\n",
    "            binary_labels.append(0)\n",
    "        else:\n",
    "            binary_labels.append(1)\n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_labels_unlabled_data(unlabeled):\n",
    "    length = 0\n",
    "    gold_labels = []\n",
    "    for sentences in unlabeled:\n",
    "        label = []\n",
    "        length += len(sentences)\n",
    "        for token in sentences:\n",
    "            if token[0].isupper() == True: # Checks if token has captial letter\n",
    "                label.append(1)\n",
    "                \n",
    "            else:\n",
    "                label.append(0) # Adds 0 if token has lower \n",
    "        gold_labels.append(label)\n",
    "    \n",
    "    return gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Reults section '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Reults section \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "norne_preds_binary = transform_to_binary(preds_norne)\n",
    "\n",
    "gold_binary = transform_to_binary(gold_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_split_sentences = split_list_preds(preds_norne, test_dataset.ner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aarne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFICATION REPORT\n",
    "norne_cr_df_all_labels = classification_report(gold_labels, preds_norne, labels = ner_vocab[:-2], digits=5, output_dict=True)\n",
    "norne_cr_IOB2 = cr(test_dataset.ner_labels, preds_split_sentences, mode='strict', scheme=IOB2, digits=4, output_dict=True)\n",
    "norne_cr_binary = classification_report(gold_binary, norne_preds_binary, digits=5, output_dict=True)\n",
    "\n",
    "norne_cr_df_all_labels = pd.DataFrame(norne_cr_df_all_labels).transpose()\n",
    "norne_cr_df_IOB2 = pd.DataFrame(norne_cr_IOB2).transpose()\n",
    "norne_cr_df_binary = pd.DataFrame(norne_cr_binary).transpose()\n",
    "\n",
    "\n",
    "## CONFUSION MATRIX\n",
    "norne_cf_matrix_no_O = confusion_matrix(gold_labels, preds_norne, labels = ner_vocab[:-2])\n",
    "norne_cf_matrix_O = confusion_matrix(gold_labels, preds_norne, labels = ner_vocab[:-1])\n",
    "norne_cr_matrix_binary = confusion_matrix(gold_binary, norne_preds_binary)\n",
    "\n",
    "norne_cf_df_no_O = pd.DataFrame(norne_cf_matrix_no_O)\n",
    "norne_cf_df_O = pd.DataFrame(norne_cf_matrix_O)\n",
    "norne_cf_df_binary = pd.DataFrame(norne_cr_matrix_binary)\n",
    "\n",
    "norne_cf_df_no_O.columns = ner_vocab[:-2]\n",
    "norne_cf_df_no_O.index = ner_vocab[:-2]\n",
    "norne_cf_df_O.columns = ner_vocab[:-1]\n",
    "norne_cf_df_O.index = ner_vocab[:-1]\n",
    "\n",
    "\n",
    "# class report\n",
    "norne_cr_df_all_labels.to_csv(\"NPSC_SAMPLES_cr_df_all_labels\", mode=\"w\")\n",
    "norne_cr_df_IOB2.to_csv(\"NPSC_SAMPLES_cr_df_IOB2\", mode=\"w\")\n",
    "norne_cr_df_binary.to_csv(\"norne_cr_df_binary\", mode=\"w\")\n",
    "\n",
    "#confusion matrix\n",
    "norne_cf_df_no_O.to_csv(\"NPSC_SAMPLES_cf_df_no_O\", mode=\"w\")\n",
    "norne_cf_df_O.to_csv(\"NPSC_SAMPLES_cf_df_O\", mode=\"w\")\n",
    "norne_cf_df_binary.to_csv(\"NPSC_SAMPLES_cf_df_binary\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parl_cr = classification_report(gold_labels_parl, y_pred_parl_binary, digits=5, output_dict=True)\n",
    "# parl_cf_matrix = confusion_matrix(gold_labels_parl, y_pred_parl_binary)\n",
    "\n",
    "# parl_cr_df = pd.DataFrame(parl_cr).transpose()\n",
    "# parl_cf_df = pd.DataFrame(parl_cf_matrix)\n",
    "\n",
    "# parl_cf_df.columns = [0, 1]\n",
    "# parl_cf_df.index = [0, 1]\n",
    "\n",
    "# parl_cr_df = pd.DataFrame(parl_cr).transpose()\n",
    "# parl_cf_df = pd.DataFrame(parl_cf_matrix)\n",
    "\n",
    "# parl_cf_df.columns = [0, 1]\n",
    "# parl_cf_df.index = [0, 1]\n",
    "\n",
    "# parl_cr_df.to_csv(\"parl_classification_report_ncrf\", mode=\"w\")\n",
    "# parl_cf_df.to_csv(\"parl_matrix_ncrf\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
